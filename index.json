[{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.7-security/5.7.1-s3-cloudfront/","title":"Configure S3 &amp; CloudFront","tags":[],"description":"","content":"1. Create S3 Bucket:\nCreate Bucket (Ex: minimarket-assets-prod) Block Public Access: On Manually upload images folder from code to this Bucket 2. Create CloudFront Distribution:\nOrigin type: Select Elastic Load Balancer Origin Domain: Select Load Balancer of Beanstalk Settings: Select Customize origin settings Protocol: HTTP Only Cache settings: Select Customize cache settings Viewer Protocol Policy: Redirect HTTP to HTTPS 3. Add S3 Origin (To retrieve images):\nGo to newly created Distribution Go to Origins tab \u0026gt; Create Origin Origin domain select the S3 created earlier (minimarket-assets-prod) Origin Access: Select Origin access control (OAC) \u0026gt; Create new OAC Bucket Policy: Copy policy provided by CloudFront and paste into S3 Bucket policy 4. Configure Behavior:\nReturn to CloudFront go to Behaviors tab Create Behavior with Path pattern: /images/ Point to Origin S3 Cache Policy: CachingOptimized "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.6-cicd/5.6.1-codebuild/","title":"Create Build Project","tags":[],"description":"","content":" Access CodeBuild \u0026gt; Create project\nProject name: MiniMarket-Build\nSource: Select GitHub (Connect to Repo containing code)\nEnvironment:\nEnvironment image: Managed Image Operating system: Amazon Linux Runtime: Standard Image: 5.0 Service role: New service role Privileged: Enable (Required to run Docker build commands) Buildspec: Use a buildspec file\nClick Create build project\nAfter creation is complete, go to IAM Role of the newly created CodeBuild, grant additional permission AmazonEC2ContainerRegistryPowerUser so it can push images to ECR\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.3-network/5.3.1-create-vpc/","title":"Create VPC &amp; Subnets","tags":[],"description":"","content":" Open Amazon VPC console (Note: Choose region suitable for needs, here the group uses Region ap-southeast-1) Select Create VPC Configuration: Name: MiniMarket-VPC IPv4 CIDR: 10.0.0.0/16 Create Subnets (Split 2 AZs to ensure High Availability): Public Subnets (2): 10.0.1.0/24 \u0026amp; 10.0.2.0/24 (Used for Load Balancer \u0026amp; NAT) Private Subnets (2): 10.0.3.0/24 \u0026amp; 10.0.4.0/24 (Used for App, DB, Redis) Click Create VPC and wait for state to change to Available is successful "},{"uri":"https://lvckio.github.io/Ckio1164/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nEvent Summary Report: “GenAI \u0026amp; Bedrock AI Services Workshop” Event Objectives This workshop helped me gain a clearer understanding of:\nHow Foundation Models are built and how they work Common Prompt Engineering techniques The architecture of Retrieval-Augmented Generation (RAG) – one of today’s most widely used AI approaches AWS pretrained AI services How to build an AI Agent using Amazon Bedrock AgentCore Speakers Lam Tuan Kiet – FPT Software (previously worked in the banking industry) Danh Hoanh Hieu Nghi – Bedrock AgentCore Specialist Dinh Le Hoang Anh – AI/ML Specialist Key Insights I Learned Foundation Models Foundation Models are trained using self-supervised learning, meaning they learn from massive amounts of unlabeled data. One model can perform many different tasks. Amazon Bedrock provides powerful models such as Titan, Luma, DeepSeek, and more. Prompt Engineering I learned that writing prompts properly can significantly improve the model’s responses.\nMain Techniques: Zero-shot prompting: asking a question without giving examples. Few-shot prompting: providing example outputs so the model can follow a specific format. Chain of Thought (CoT): guiding the model to reason step by step, helping it produce more logical and accurate answers. I found Chain of Thought especially interesting because it allows the model to explain how it arrived at its answer.\nRetrieval-Augmented Generation (RAG) This was my favorite part because it is widely used in real-world applications, especially in banking.\nRAG consists of 3 main steps: Retrieval – finding relevant information Augmentation – adding this information into the prompt Generation – producing the answer using a Large Language Model (LLM) Embedding Converts text into vectors for semantic search Amazon Titan Embedding supports many languages How RAG Works Documents → chunking → embedding → vector store\nWhen a user asks a question:\nThe question is embedded Similar information is retrieved from the vector store The context is added to the prompt The model generates a more accurate answer I realized that the vector store is a crucial part of the entire RAG pipeline.\nAWS AI Services Introduced in the Workshop Amazon Rekognition Image and video analysis Face and object detection Used in security and camera systems Amazon Translate Real-time and batch text translation Amazon Textract Extracts text from invoices, forms, and documents Amazon Polly Converts text into natural-sounding speech Amazon Comprehend Language analysis: sentiment, key phrases, PII detection, etc. Amazon Kendra Intelligent search engine with semantic search and RAG support Amazon Personalize Personalized recommendations (similar to Netflix or Shopee) Additional tools include the Lookout services, Transcribe, and Pipecat for real-time AI interactions.\nAmazon Bedrock AgentCore This part was presented by Mr. Nghi, and it helped me understand how to create a complete AI agent.\nSupported Ecosystem LangGraph LangChain Strands Agent SDK Agent Development Workflow From idea → development → real-world deployment With a focus on:\nPerformance Scalability Security Governance AgentCore Components Runtime Memory Identity Gateway Code Interpreter Browser Tool Observability I think AgentCore is very powerful because it allows AI agents to run reliably in enterprise environments.\nKey Takeaways Foundation Models are extremely capable of handling multiple tasks Prompt engineering is more important than I expected RAG is ideal for building internal chatbots since it ensures factual and accurate answers AWS AI services help reduce development time significantly AgentCore supports building production-level AI agents Applications to My Work and Studies I can apply Few-shot + CoT when solving assignments or building AI applications I want to try building a RAG chatbot to search documents for my class or workplace Textract + Comprehend can help automate document processing tasks AgentCore seems perfect for building an AI assistant for workflow automation My Experience at the Workshop I learned many practical AI concepts that I previously only saw in theory The speakers explained everything clearly with real examples I now understand how large companies implement AI for their customers This workshop helped broaden my thinking and guide my future career direction in AI Event Photos "},{"uri":"https://lvckio.github.io/Ckio1164/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nEvent Summary Report: “Monitoring \u0026amp; Observability on AWS Workshop” 1. Event Objectives This workshop helped me gain a better understanding of:\nThe difference between Monitoring and Observability AWS services used for system monitoring and visibility How to use Amazon CloudWatch to track logs, metrics, alarms, and dashboards An introduction to AWS X-Ray and how it helps analyze microservices performance 2. Introduction to Monitoring and Observability Monitoring Monitoring is the process of tracking the system through logs, metrics, events, etc. It shows the current state of the system in real time. Useful for DevOps \u0026amp; SRE teams to detect issues early. Observability Observability goes beyond monitoring. It helps us understand why a problem happened, not just what happened. Focuses on answering: “What is happening inside the system and why?” Monitoring \u0026amp; Observability on AWS The workshop introduced three main AWS tools:\nAmazon CloudWatch – logs, metrics, alarms, dashboards Amazon Managed Grafana – advanced visualization and analytics AWS X-Ray – distributed tracing for microservices 3. Amazon CloudWatch 3.1 CloudWatch Overview I learned that CloudWatch is not just a monitoring tool, but a full observability platform:\nMonitors resources and applications in real time Collects metrics and logs for deeper analysis Supports alarms and automated responses, helping DevOps \u0026amp; SRE workflows Provides dashboards for operational insights and cost optimization 3.2 CloudWatch Metrics A metric is data that represents the performance of the system (CPU, RAM, latency, error rate, etc.) CloudWatch collects default AWS metrics automatically We can create custom metrics or collect data from on-premises systems using the CloudWatch Agent Metrics integrate easily with other AWS services (Lambda, ECS, API Gateway, etc.) 3.3 CloudWatch Logs Stores and analyzes application logs Allows searching, filtering, and creating log-based metrics Helps detect errors, track user behavior, and debug applications 3.4 CloudWatch Alarms Used to trigger alerts based on metric thresholds\nWhen triggered, alarms can:\nSend notifications via SNS Automatically scale resources Trigger Lambda functions to handle incidents 3.5 CloudWatch Dashboards Provides a visual view of system health Supports custom charts and graphs Useful for DevOps teams to monitor systems at a glance 4. AWS X-Ray AWS X-Ray was introduced as a powerful tool for observing and debugging microservices-based systems.\n4.1 Distributed Tracing Tracks an entire request path end-to-end Generates service maps to visualize how microservices interact Requires integrating the X-Ray SDK to produce trace IDs 4.2 Performance Analysis Helps identify:\nPerformance bottlenecks Slow services Failures inside microservice calls 4.3 Integration with CloudWatch X-Ray traces can be combined with CloudWatch metrics and logs Provides a more complete picture of system behavior Helps developers fix issues faster 5. Key Takeaways Monitoring and observability are different—observability helps understand why issues happen CloudWatch is very powerful because it supports metrics → logs → alarms → dashboards X-Ray is extremely useful for monitoring microservices and request flows Combining CloudWatch and X-Ray creates a strong observability stack for any application 6. Applications to Work and Study I can use CloudWatch to monitor my AWS lab projects For microservices applications, I want to try integrating X-Ray to visualize request flows Dashboards can help me present system status in reports Alarms allow automated detection of issues without manual monitoring 7. My Experience at the Event I learned practical system monitoring techniques that I had only seen in theory before The demos of metrics, alarms, and X-Ray made everything easier to understand This knowledge is very useful if I want to pursue DevOps or Cloud Engineering The workshop helped broaden my understanding of system observability "},{"uri":"https://lvckio.github.io/Ckio1164/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Lam Vinh Cuong\nPhone Number: 0903563176\nEmail: CuongLVSE182626@Fpt.edu.vn\nUniversity: FPT University Major: Asurrance Information\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction MiniMarket is an e-commerce application built on the .NET Core platform, applying modern 3-Tier Architecture. The goal of this Workshop is to re-platform the application from an On-premise environment to AWS cloud infrastructure (Cloud Native Migration) ensuring AWS Well-Architected Framework criteria: Security, Reliability, Performance Efficiency, and Cost Optimization Workshop Overview Solution Architecture:\nCompute: Use AWS Elastic Beanstalk (Docker platform) to simplify deployment, infrastructure management, and Auto Scaling Database: Amazon RDS for SQL Server deployed in Private Subnet to ensure data security Caching: Amazon ElastiCache (Redis) helps store User Sessions and offload Database queries, increasing response speed Network \u0026amp; Security: VPC: Designed with Public/Private Subnet model combined with NAT Gateway Application Layer Security: Use AWS WAF combined with Amazon CloudFront to protect against Web attacks and distribute content globally Storage: Amazon S3 used to store and serve static assets (product images) with high durability DevOps: Fully automated CI/CD process with AWS CodePipeline and CodeBuild Monitoring: Amazon CloudWatch to monitor system health (CPU, Network) and send alerts "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.8-monitoring/5.8.1-cloudwatch/","title":"Monitoring with CloudWatch","tags":[],"description":"","content":" Create SNS Topic:\nGo to SNS \u0026gt; Topics \u0026gt; Create Topic Type: Standard Name: DevOps-Alerts Create Subscription\nCreate Subscription \u0026gt; Protocol: Email \u0026gt; Enter your email (Remember to Confirm mail) Create CPU Alarm:\nGo to CloudWatch \u0026gt; Alarms \u0026gt; Create alarm Select metric \u0026gt; EC2 \u0026gt; Per-Instance Metrics \u0026gt; Select InstanceID of Beanstalk \u0026gt; CPUUtilization Condition: CPUUtilization: Greater than 70% Notification: Select Topic DevOps-Alerts Create Alarm. "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.5-app/5.5.1-dockerize/","title":"Package with Docker","tags":[],"description":"","content":"Before moving to Cloud, we need to package the .NET Core application into a Docker Image\nCreate Dockerfile: At the root directory of the Solution, create a file named Dockerfile (no extension) ```dockerfile # STAGE 1: BUILD FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build WORKDIR /src COPY [\u0026quot;MiniMarket.sln\u0026quot;, \u0026quot;./\u0026quot;] COPY [\u0026quot;WebShop/WebShop.csproj\u0026quot;, \u0026quot;WebShop/\u0026quot;] # ... (Copy other projects if any) RUN dotnet restore \u0026quot;MiniMarket.sln\u0026quot; COPY . . WORKDIR \u0026quot;/src/WebShop\u0026quot; RUN dotnet build \u0026quot;WebShop.csproj\u0026quot; -c Release -o /app/build RUN dotnet publish \u0026quot;WebShop.csproj\u0026quot; -c Release -o /app/publish # STAGE 2: RUNTIME FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final COPY --from=build /app/publish . WORKDIR /app EXPOSE 8080 ENTRYPOINT [\u0026quot;dotnet\u0026quot;, \u0026quot;WebShop.dll\u0026quot;] ENV ASPNETCORE_URLS=http://+:8080 ENV ASPNETCORE_ENVIRONMENT=Development ``` Create buildspec.yml: Create file buildspec.yml to instruct AWS CodeBuild how to package and push to ECR ```yaml version: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... # --- INFORMATION CONFIGURATION --- - AWS_DEFAULT_REGION=ap-southeast-1 # Replace your Account ID in the line below: - AWS_ACCOUNT_ID= YOUR ACCOUNT ID - IMAGE_REPO_NAME=market-app - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME # --------------------------- - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com build: commands: - echo Build started on `date` - echo Building the Docker image... - docker build -t $REPOSITORY_URI:latest . - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - echo Pushing the Docker image... - docker push $REPOSITORY_URI:latest - docker push $REPOSITORY_URI:$IMAGE_TAG - echo Writing image definitions file... # Automatically create Dockerrun.aws.json configuration file for Beanstalk # Map Port 80 (Host) to 8080 (Container .NET) - printf '{\u0026quot;AWSEBDockerrunVersion\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;Image\u0026quot;:{\u0026quot;Name\u0026quot;:\u0026quot;%s\u0026quot;,\u0026quot;Update\u0026quot;:\u0026quot;true\u0026quot;},\u0026quot;Ports\u0026quot;:[{\u0026quot;ContainerPort\u0026quot;:8080,\u0026quot;HostPort\u0026quot;:80}]}' \u0026quot;$REPOSITORY_URI:$IMAGE_TAG\u0026quot; \u0026gt; Dockerrun.aws.json - cat Dockerrun.aws.json artifacts: files: - Dockerrun.aws.json ``` "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.9-cleanup/5.9.1-cleanup/","title":"Resource Cleanup","tags":[],"description":"","content":"To avoid unexpected costs after completing the Workshop, delete resources in the following correct order:\nNAT Gateway: Delete NAT Gateway \u0026gt; Wait for Deleted \u0026gt; Release Elastic IP (Most important as it costs the most) Elastic Beanstalk: Terminate Environment ElastiCache: Delete Redis Cluster (Uncheck Create Backup) RDS: Stop (or Delete if no longer in use - remember to uncheck Final Snapshot). WAF: Manage resources \u0026gt; Disassociate \u0026gt; Delete protection pack (web ACL) S3: Empty and Delete Bucket (Can skip deleting if still in use as cost is not too high)\nCloudFront Disable and Delete\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.4-data/5.4.1-security-groups/","title":"Set up Security Groups","tags":[],"description":"","content":" Access EC2 \u0026gt; Security Groups \u0026gt; Create security group Group 1: Web Server (sg-web-app) Description: Allow HTTP from Internet Inbound Rules: Type: HTTP (80) | Source: 0.0.0.0/0 (Or only from Load Balancer if wanting stricter security) Group 2: Database (sg-db-sql) Description: Allow access only from Web Server Inbound Rules: Type: MSSQL (1433) | Source: Custom \u0026gt; Select ID of sg-web-app Group 3: Redis Cache (sg-redis-cache) Description: Allow access only from Web Server Inbound Rules: Type: Custom TCP (6379) | Source: Custom \u0026gt; Select ID of sg-web-app "},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Get acquainted with FCJ members - Read rules and regulations - Create AWS free tier account - Get familiar with AWS Console interface - Finished module 1 09/09/2025 09/09/2025 https://policies.fcjuni.com/ 4 - Read guide and installed neccessary extensions to create a site for worklog and others in the future - Started Module 2: + Learned about AWS Virtual Private Cloud + Understand the concept of VPC + Get to know about VPC Security and others VPC feature 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Watched Module 2 lab 3 to get an overview of the topic - Practiced before implementation 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Launched EC2 Instance - Testing Amazon Bedrock Playground feature - Created a web using Lambda service - Created a database with RDS service for testing purpose 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nLearned how to reduce costs by choosing AWS services that align with specific project requirements\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Understand foundational AWS concepts.\nCreate and configure an AWS account.\nBecome familiar with IAM and AWS CLI usage.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to internship roadmap - Learn AWS fundamentals \u0026amp; global infrastructure 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn AWS Support models - Study AWS Pricing \u0026amp; Budgets - Take notes on cost-management best practices 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn IAM basics : + Users + Groups + Policies - IAM security best practices 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Install AWS CLI - Configure CLI (Access key, secret key, default region) - Execute basic AWS CLI commands 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice : + Create IAM users\n+ Create a simple policy + Manage AWS resources using both Console \u0026amp; CLI 09/19/2025 09/19/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Gained a solid understanding of AWS fundamentals.\nLearned how AWS Support and AWS Budgets work.\nMastered IAM basics:\nUser\nGroup\nRole\nPolicy\nSuccessfully installed and configured AWS CLI: Access Key\nSecret Key\nDefault Region\nPerformed essential CLI commands: Check configuration\nList IAM users\nList regions\nManaged AWS resources via both the Console \u0026amp; CLI efficiently.\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Understand AWS Networking fundamentals. Learn core EC2 concepts. Practice launching \u0026amp; managing compute resources on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn VPC basics +Subnets +Route Tables +nternet Gateway 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Network Security: + Security Groups + NACLs - NAT Gateway vs IGW 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Amazon EC2 basics: + Instance Types + AMI + EBS + Key Pair 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Elastic IP overview - SSH methods to connect to EC2 - Configure SG for secure access 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + SSH into the server + Create \u0026amp; attach an EBS volume 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Gained solid understanding of VPC structure.\nUnderstood network security using SGs \u0026amp; NACLs.\nMastered EC2 essentials: AMI, EBS, instance types, key pairs.\nSuccessfully connected to EC2 via SSH.\nLaunched EC2, attached additional EBS volume, configured Elastic IP.\nAcquired the fundamental skills needed to operate servers on AWS.\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Get familiar with AWS Cloud9 as a cloud-based IDE. Learn how to host a static website using Amazon S3. Understand and practice using Amazon RDS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to AWS Cloud9 -Create and configure a cloud development environment 09/29/2025 09/29/2025 3 - Learn Amazon S3 + Bucket + Objects + Storage classes - Understand S3 security \u0026amp; permissions 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Static website hosting on S3 - Configure Bucket Policy for public access 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Amazon RDS fundamentals + Engine + Backup + High Availability 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create an RDS instance + Connect RDS from EC2 + Configure security groups 10/03/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Used AWS Cloud9 to code directly on a cloud IDE.\nLearned core S3 components:\nBucket\nObjects\nStorage Classes\nBucket Policies\nSuccessfully deployed a static website on Amazon S3.\nUnderstood RDS concepts including:\nDatabase engines\nAutomated backups\nMulti-AZ high availability\nConnected EC2 to RDS through proper security configuration.\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 -Learn Amazon Lightsail - Create \u0026amp; configure a simple instance 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 3 - Lightsail networking - Snapshots and backup 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Lightsail Containers - Deploy a test container 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - EC2 Auto Scaling concepts: + Launch Template + AMI + EBS - SSH connection methods to EC2 - Learn about Elastic IP 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create ASG + set scaling policies + simulate scale-out / scale-in 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Deployed applications using Lightsail \u0026amp; Lightsail Containers. Understood snapshots, backups, and networking on Lightsail. Created EC2 Auto Scaling Groups and tested scaling behavior. "},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Learn system monitoring with CloudWatch. Understand DNS \u0026amp; Route 53. Master DynamoDB basics. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - CloudWatch basics : + Metrics \u0026amp; Logs 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ 3 - Create Dashboards \u0026amp; send custom logs 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Route 53: + Hosted zones + Routing policies 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 -DynamoDB fundamentals: + PK + SK 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - DynamoDB indexes: + GSI + LSI + Query + Scan 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Gained knowledge of CloudWatch monitoring tools. Built dashboards and managed logs. Understood Route 53 routing strategies. Mastered DynamoDB design \u0026amp; indexing. Performed complex queries and data analysis "},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Learn ElastiCache (Redis). Learn CloudFront and CDN. Lab Networking on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - ElastiCache Redis basics 10/20/2025 10/20/2025 3 - AWS Networking Workshop 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ 4 - CloudFront fundamentals 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/ 5 - Lambda@Edge concepts 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ 6 - S3 + CloudFront Deploymen 10/24/2025 10/24/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Learned caching strategies with Redis. Deployed global content delivery with CloudFront. Applied edge computing with Lambda@Edge. "},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Windows workloads on AWS. AWS Directory Services. Building a High Availability system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Finish:Introduction to Research for Essay Writing 10/27/2025 10/27/2025 https://coursera.org/share/6ed08292233380ad1eb1b9174ed94da6 3 - Finish: Research Methodologies 10/28/2025 10/28/2025 https://coursera.org/share/18f9264d9fe1096e833ea894b7ee8425 4 - Finish: Being a Researcher (Information Science) 10/29/2025 10/29/2025 https://coursera.org/share/add306ac487d15f5430837e072aefd0d 5 - Finish: Advanced Writing Techniques 10/30/2025 10/30/2025 https://coursera.org/share/ba0e45285a73c01105918df9b6d5e970 6 - Complete all quizzes \u0026amp; course assessments Review 10/31/2025 10/31/2025 Week 8 Achievements: Deployed directory services. Designed a highly available system. "},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Migration Overview 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 3 - VM Import/Export 11/04/2025 11/04/2025 https://cloudjourney.awsstudygroup.com/ 4 - AWS DMS 11/05/2025 11/05/2025 https://cloudjourney.awsstudygroup.com/ 5 - AWS SCT 11/06/2025 11/06/2025 https://cloudjourney.awsstudygroup.com/ 6 - Disaster Recovery 11/07/2025 11/07/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.3-network/5.3.2-gateways/","title":"Configure Internet &amp; NAT Gateway","tags":[],"description":"","content":"Create Internet Gateway In the VPC dashboard click on Internet gateways Then click on Create internet gateway In the Internet gateway creation section, name it as desired then click on Create internet gateway and wait for it to be created After the Internet gateway is finished creating go to Actions and click on Attach to VPC to attach it to the VPC created in the previous section Create NAT Gateway Create NAT Gateway placed in Public Subnet 1 Assign Elastic IP to have a static address to the Internet "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.3-network/5.3.3-routing/","title":"Configure Route Table","tags":[],"description":"","content":"Create Route Table Click on Route tables section in VPC dashboard\nCreate 2 Route Tables, Public with Private\nPublic Route Table: For Public Route Table, in Routes section click Edit routes Point 0.0.0.0/0 to Internet Gateway And in Subnet associations section, assign to both Public Subnets Private Route Table: For Private Route Table, we will point 0.0.0.0/0 to NAT Gateway And in Subnet associations section, assign to both Private Subnets Separating Route Tables ensures that Databases in Private Subnet are never exposed directly to the Internet.\n"},{"uri":"https://lvckio.github.io/Ckio1164/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"“DevOps on AWS” Report Event Purpose Guide career paths in DevOps and Cloud fields. Deeply understand CI/CD processes and Containerization. Analyze the role of Infrastructure as Code (IaC) versus ClickOps. Compare Orchestration solutions on AWS (ECS vs EKS) and Monitoring/Observability strategies. Highlights Next-Generation DevOps Roadmap Related roles: DevOps Engineer, Cloud Engineer, Platform Engineer, Site Reliability Engineer (SRE). T-shaped Skill: A skill development model with broad knowledge across many areas and deep knowledge in one specific area. Advice for beginners: Do: Start with Fundamentals, learn through Real Projects, Document everything, focus on mastering one skill at a time. Don\u0026rsquo;t: Get stuck in \u0026ldquo;Tutorial Hell\u0026rdquo;, copy-paste blindly, compare yourself to others, give up after failures. Continuous Integration \u0026amp; Deployment (CI/CD) CI (Continuous Integration): The process of frequently integrating code into a shared repository. Distinguishing CD: Continuous Delivery: Automated up to Acceptance Test; deployment to Production requires a manual trigger. Continuous Deployment: Fully automated from code to Production. Infrastructure as Code (IaC) Problems with ClickOps: Slow, prone to Human Error, inconsistent, difficult to collaborate. Benefits of IaC: Automation, Scalability, Reproducibility, enhanced collaboration. AWS CloudFormation: AWS\u0026rsquo;s built-in IaC tool using JSON/YAML templates. Stack: A collection of resources managed together. Drift Detection: Detects discrepancies between actual configuration and the template (when someone manually modifies resources). AWS CDK (Cloud Development Kit): Uses programming languages (Python, TS\u0026hellip;) to define infrastructure. Concepts: L1 (Mapping 1:1), L2, L3 Constructs. CLI commands: cdk init, cdk synth, cdk deploy, cdk diff, cdk destroy. Terraform: Open-source tool, supports Multi-cloud, uses HCL language, manages state via State file. Container Ecosystem Docker Fundamentals: Dockerfile (build definition) → Image (packaging blueprint) → Container (runtime). Amazon ECR: AWS\u0026rsquo;s private container registry, supporting image scanning, immutable tags and lifecycle policies. Container Orchestration: Manages container lifecycle (restart, scale, distribute traffic). Amazon ECS: AWS native solution. Supports Launch types: EC2 (server management) and Fargate (Serverless - easier). Amazon EKS: Managed Kubernetes service. Suitable for complex systems requiring K8s experience. Amazon App Runner: Simplest solution to deploy web apps/APIs quickly and cost-effectively. Monitoring \u0026amp; Observability Distinction: Monitoring: Tracking Logs, Metrics (How is the system performing?). Observability: Deeply understanding root causes (Why is the system performing this way?). Amazon CloudWatch: Collects Metrics (CPU, RAM, Network\u0026hellip;) and Logs in real-time. Alarms: Alerts and automated responses. Dashboards: Visualize operational data. AWS X-Ray: Distributed tracing for microservices, helping visualize service maps and analyze performance bottlenecks. What I Learned System Thinking Infrastructure Drift: Understanding the risks of manual resource modification outside of IaC and the importance of using Drift Detection. Trade-offs: Knowing how to select IaC tools (CDK for AWS-centric, Terraform for Multi-cloud) and Orchestration tools (ECS for beginners/simple needs, EKS for advanced features). Operational Skills Standard Process: The core difference between Continuous Delivery and Continuous Deployment lies in the manual approval step to Production. Container Management: Understanding the workflow from Dockerfile to ECR and how ECS/EKS orchestrates container operations. Application to Work Transition to IaC: Start writing CloudFormation or CDK for current projects instead of operating on the Console. Optimize Pipeline: Review CI/CD processes, integrate automated testing steps before deployment. Implement Observability: Integrate AWS X-Ray into applications to trace requests across microservices, combined with CloudWatch Alarms for proactive monitoring. Refactor Docker: Optimize Dockerfiles and use ECR Lifecycle Policies to manage image storage space. Event Experience Participating in the “Next-Generation DevOps \u0026amp; Cloud Architecture” event was a crucial stepping stone helping me clearly define my DevOps skill development path.\nClear Orientation The speaker outlined a very practical learning Roadmap with the advice \u0026ldquo;Don\u0026rsquo;t stay in Tutorial Hell\u0026rdquo; - which resonated with me. The T-shaped skill model helped me realize I don\u0026rsquo;t need to know everything at once but should focus deeply on one area first. In-depth Tool Knowledge The detailed comparison between ECS and EKS made me more confident in choosing the right compute solution for my project (as a beginner, ECS Fargate is the optimal choice). The presentation on IaC and Drift Detection completely changed my mindset on infrastructure management: \u0026ldquo;Use code, not clicks\u0026rdquo;. The Importance of Observability I realized that Monitoring (looking at charts) is not enough; achieving Observability (understanding the nature) through tools like AWS X-Ray is necessary to thoroughly resolve issues in distributed systems. Some photos from the event Add your photos here This event not only provided technical knowledge but also inspired a professional mindset, from building CI/CD culture to automating everything possible.\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.4-data/5.4.2-rds/","title":"Initialize Amazon RDS","tags":[],"description":"","content":" Access RDS Console \u0026gt; Subnet groups \u0026gt; Create DB subnet group Name: db-private-group Subnets: Select 2 AZs and select exactly 2 Private Subnets Go to Databases \u0026gt; Create database Engine options: Microsoft SQL Server (Express Edition) Templates: Free tier Settings: Set Master Password (remember for later use) Connectivity: VPC: VPC you created for Web Subnet group: db-private-group Public access: No VPC security group: Select Security group you created for database Click Create database "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.5-app/5.5.2-beanstalk-setup/","title":"Initialize Elastic Beanstalk","tags":[],"description":"","content":"We will create an environment to run the application\nAccess Elastic Beanstalk \u0026gt; Create application App Name: MiniMarket-App Platform: Docker (Amazon Linux 2023) Application code: Select Sample application (To test infrastructure first) Network Configuration (Networking) - Extremely Important: VPC: Select the VPC you created for MiniMarket Instance settings: Public IP address: Uncheck Subnets: Select 2 Private Subnets EC2 security groups: Select sg-web-app Capacity: Environment type: Select Load balanced Load balancer network settings: Visibility: Public Subnets: Select 2 Public Subnets Click Create. The system will take about 5-7 minutes to initialize "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM permissions Attach AdministratorAccess in IAM permission policy to the AWS account for easier workflow Note: Using Administrator privileges is recommended only for the Workshop environment to ensure the deployment process is uninterrupted. In a real Production environment, adhere to the Least Privilege principle for each service { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Source Code GitHub repository containing .NET Core code and a valid Dockerfile "},{"uri":"https://lvckio.github.io/Ckio1164/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Digital Transformation for Mini-market on AWS Cloud Platform .NET 3-tier E-commerce Solution applying Repository and Unit of Work Pattern 1. Executive Summary This proposal presents an end-to-end solution for \u0026ldquo;Digital Transformation for Mini-market on AWS Cloud Platform\u0026rdquo;. Traditional mini-markets are currently facing three major challenges: (1) Manual inventory management (using Excel/notebooks) causing revenue loss and resource waste; (2) 100% dependence on offline sales channels, missing the growing e-commerce market and losing competitiveness; and (3) Slow operational processes (such as manual price lookup), resulting in poor customer experience.\nThe group\u0026rsquo;s solution is to build a comprehensive e-commerce and operations management platform. Regarding software architecture, the group will use .NET 3-tier architecture (ASP.NET Core MVC, EF Core) combined with Repository Pattern and Unit of Work Pattern. Regarding infrastructure architecture, it is designed according to the AWS Well-Architected Framework, running on AWS Elastic Beanstalk (for .NET backend), Amazon RDS for SQL Server (for database), and Amazon S3 (for static assets). The system is performance-optimized using CloudFront and ElastiCache, and secured using WAF, VPC, and NAT Gateway. The deployment process is fully automated using a CI/CD pipeline integrated with GitHub.\nBusiness benefits are immediate, including automated inventory management (reducing loss) and opening a new online revenue channel. Regarding investment, infrastructure costs in the first 12 months are nearly zero by leveraging AWS Free Tier (e.g., RDS Express Edition, EC2 t3.micro). Long-term operating costs (after Free Tier) are also very practical, estimated at around 138.06 USD/month for the entire system. With minimal initial investment and the ability to directly solve problems causing revenue loss, ROI is very high and almost instant.\nThe project is proposed to be implemented in 11 weeks, divided into 4 main phases: (1) Foundation \u0026amp; Architecture, (2) Core Feature Development, (3) AWS Integration \u0026amp; CI/CD, and (4) Finalization \u0026amp; Deployment. Expected results are measured by specific success metrics: reduce inventory errors by 90%, reduce checkout time by 50%, and achieve 20% revenue from online channels in the first 6 months. This solution not only solves immediate problems but also provides mini-markets with a scalable platform for data-driven decision-making in the future.\n2. Problem Statement Current Problem\nSmall and medium retail businesses, especially the traditional \u0026ldquo;mini-market\u0026rdquo; model in Vietnam, are operating based on outdated manual processes. In the context of an increasingly digitized market, failing to apply technology to the work environment has created several problems, directly affecting their survival and growth.\nKey Problems\nManual inventory management leads to inaccuracy in figures and resource waste: Most mini-markets currently manage thousands of product codes (SKUs) using notebooks or Excel files. Importing/exporting goods and end-of-day inventory checks rely entirely on manual counting and entry. This can lead to data errors because the manual entry process is prone to mistakes, typically product codes and quantities, causing significant discrepancies between \u0026ldquo;on book\u0026rdquo; data and \u0026ldquo;actual\u0026rdquo; stock. Checking goods manually like this also demands high resources as staff have to spend hours every day counting, reconciling, and correcting reports, instead of focusing on sales or customer service. Finally, financial loss occurs; when data is not entered accurately, store owners cannot control the condition of goods such as expired goods, damaged goods, or theft, leading to a loss of 5-10% of inventory value monthly. Dependence on offline sales, missing the E-commerce market: Typically, mini-markets in Vietnam mostly depend on walk-in customers (offline). They are also limited by geographical location (only serving the local area) and a familiar customer base. Stores like this stand outside the E-commerce market, missing the young customer base already accustomed to online shopping. They cannot compete on convenience such as 24/7 ordering or door-to-door delivery compared to large convenience store chains like Circle K or 7-Eleven and delivery apps like Grab and Shopee, leading to a risk of losing customers over time. Operational and customer experience issues: The payment and information lookup process in traditional mini-markets is usually very slow. When customers ask about price, product information, or promotions, staff (especially new staff) have to manually look up in notebooks, leading to wasted time. Making customers wait long for information lookup or checkout creates frustration and unprofessionalism. Staff waste too much time on simple, error-prone tasks (such as misreading prices due to bad handwriting), reducing the number of customers that can be served during peak hours. 3. Solution Architecture The architecture is designed to solve the stated problems by combining .NET 3-tier software architecture with AWS Managed Services. This architecture adheres to the principles of the AWS Well-Architected Framework, ensuring security, high performance, fault tolerance, and cost optimization.\nAWS Services Used\nAWS Elastic Beanstalk: PaaS (Platform as a Service) selected to deploy the .NET 3-tier application (including WebShop Presentation Layer and Application Services Layer). Beanstalk automates 100% of infrastructure management, including automatically creating Auto Scaling Group (ASG) to ensure scalability and cost savings.\nAmazon RDS (SQL Server): Database Service (Managed Relational Database Service) to host the Persistence Layer. SQL Server was chosen because the group\u0026rsquo;s .NET application was developed and optimized for SQL Server. Using RDS for SQL Server allows migrating the application to AWS without changing code in the Data Layer. RDS will also automate complex tasks such as daily backups, patching, and failover. regarding security, RDS is placed in a Private Subnet, inaccessible directly from the Internet, only allowing the application on Beanstalk to connect. And regarding cost optimization, to optimize costs in the initial phase, we can start with SQL Server Express Edition on RDS, this version is within the AWS Free Tier.\nAmazon S3: Object Storage Service. Used to store static assets such as product images, CSS files, and JavaScript. S3 provides extremely low cost and unlimited scalability.\nAmazon CloudFront: Content Delivery Network Service - CDN. CloudFront caches static files from S3 at servers (Edge Locations) globally, helping users load pages significantly faster. It helps reduce direct load on Beanstalk servers and helps the .NET application focus on processing logic.\nAmazon WAF and Route 53: WAF (Web Application Firewall) and Route 53 (DNS Service). WAF is associated with CloudFront to block common web attacks (such as SQL injection, XSS). Route 53 provides domain names for users.\nAmazon ElastiCache (Redis): In-memory data stores service. Helps maximize load reduction for RDS Database when there are repetitive queries (for example: retrieving the homepage product list). The .NET application will cache these \u0026ldquo;hot\u0026rdquo; data on ElastiCache, helping increase response speed. Similar to RDS, ElastiCache is also placed in a Private Subnet to ensure safety.\nNAT Gateway: Network Address Translation Service. NAT will provide secure Internet access for services in the Private Subnet (like Elastic Beanstalk). This allows servers to download security patches without being accessed directly from outside.\nAWS CodePipeline/CodeBuild: Continuous Integration and Deployment (CI/CD) services. These services are integrated with GitLab to automate the process: (1) CodeBuild builds .NET code, (2) CodePipeline deploys the new version to Elastic Beanstalk.\nData Flow\n[1]-[2] Users access the domain name (via Route 53) and are routed to CloudFront. Amazon WAF will filter this request.\n[3] (Static Flow) If it is a static file (image, css), CloudFront retrieves directly from Amazon S3.\n[4]-[6] (Dynamic Flow) If it is a dynamic request, CloudFront forwards via Internet Gateway to Application Load Balancer, then ALB sends the request to Elastic Beanstalk.\n[7]-[8] The .NET application (on Beanstalk) will check ElastiCache first, if not found, will query Amazon RDS.\n[9]-[10] When Elastic Beanstalk needs Internet access (to download patches), it will go through NAT Gateway then out to Internet Gateway.\n[11]-[14] (CI/CD Flow) When Dev pushes code to Github, CodePipeline and CodeBuild will automatically build and deploy the new version to Elastic Beanstalk.\n4. Technical Implementation Implementation Stages\nThe project will be divided into 4 main stages, lasting 11 weeks to ensure progress and quality:\nBuilding Technical Foundation: Focus on building the technical foundation, including finalizing the data model for main entities, setting up .NET 3-tier solution structure (Domain, Application, Persistence, WebShop), initializing repository on Github, and researching AWS services. (Weeks 1-4)\nBuilding Core Features: Complete Persistence Layer (Repositories, Unit of Work) and Application Layer (Services) for main tasks such as managing products, users, and orders. Simultaneously, WebShop Layer (Controllers, Views) will be built for login flows, shopping cart, payment, and start writing Unit Tests for Services. (Weeks 5-7)\nIntegrating AWS Services: Integrate Amazon S3 for product images, ElastiCache (Redis) for caching. The group will also complete the CI/CD Pipeline to automatically deploy to the Staging environment on Elastic Beanstalk and perform Integration Testing. (Weeks 8-10)\nFinalization and Deployment: Configure security services such as CloudFront, WAF, and Route 53. Then, deploy version 1.0 to Elastic Beanstalk, perform final UAT, and set up basic monitoring via CloudWatch. (Week 11)\nTechnical Requirements\nBackend: ASP.NET Core MVC 9.0. ORM: Entity Framework Core. Database: MS SQL Server 2022 (Local) and Amazon RDS for SQL Server (Cloud). Frontend: Bootstrap 5, jQuery, and Bootstrap Icons. Cloud Platform (AWS): Elastic Beanstalk, RDS, S3, CloudFront, WAF, Route 53, ElastiCache, VPC, NAT Gateway, CodePipeline, CodeBuild. Source Control: Git. Tools: Visual Studio 2022, Docker Desktop. Development Methodology\nApply Agile methodology (Scrum-like) to flexibly adjust according to requirements and ensure progress, adhering to the 4 proposed deployment stages. All work (features, bugs) will be tracked and managed via Kanban board, helping the group easily grasp the progress of each task (e.g., To Do, In Progress, Done). All new code must be reviewed via merge requests on Github before being merged into the main branch, ensuring consistent code quality.\nTesting Strategy\nTo ensure quality and stability, the group will perform 3 levels of testing. First is Unit Testing, focusing 100% on the Application Layer (e.g., ProductService, OrderService) by mocking repositories to isolate Business Logic, using standard .NET testing frameworks. The second level is Integration Testing, performed on the Staging environment (on Elastic Beanstalk) to check the interaction between the Application Layer and Persistence Layer (EF Core) with the real Amazon RDS Database. Finally, User Acceptance Testing will be performed on the Production environment for the group to check complete functional flows on the user interface such as \u0026ldquo;Register, Login, Payment\u0026rdquo;.\nDeployment Plan\nApply fully automated CI/CD process. The process is automatically triggered whenever Dev pushes code to Github. Github will send a webhook triggering AWS CodePipeline, this service will take the code and command AWS CodeBuild to compile the .NET project, run Unit Tests, and package the application into a .zip file. If CodeBuild succeeds, CodePipeline will take the .zip file and automatically deploy this new version to the Staging environment on Elastic Beanstalk.\n5. Roadmap \u0026amp; Milestones The project is planned to be executed in 11 weeks, divided into 4 main stages. This schedule ensures time for development, integration, and thorough testing.\nPhase 1 (Weeks 1 - 4): This stage focuses on building the technical foundation, including finalizing data models, setting up .NET 3-tier Solution Architecture, initializing Github Repository, and researching AWS services. The milestone of this stage is Solution Architecture and Repository established, along with AWS environment (VPC, Subnets).\nPhase 2 (Weeks 5 - 7): After Phase 1 is complete, the group will build core features, complete Persistence and Application Layers (Product Management, Orders) and basic feature flows on WebShop (Auth, Cart). The milestone is main feature flows (Login, View Product, Cart, Payment) operating stably locally, and Unit Tests for Services.\nPhase 3 (Weeks 8 - 10): Dependent on Phase 2, this stage will integrate AWS services like Amazon S3 for product images and ElastiCache (Redis) for caching. The milestone is CI/CD pipeline operational, successfully automatically deploying to Staging environment on Elastic Beanstalk and Integration Testing completed.\nPhase 4 (Week 11): This final stage focuses on finalization and deployment, dependent on the stable Staging build from Phase 3. The group will configure security services (CloudFront, WAF, Route 53). The milestone is Version 1.0 successfully deployed to Production environment (Elastic Beanstalk), final User Acceptance Testing completed, and system monitored via CloudWatch.\n6. Budget Estimation Costs can be viewed on AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Costs\nEstimates from AWS Pricing Calculator show the monthly operating cost of this architecture is 138.06 USD, with an upfront cost of 0.00 USD. The group\u0026rsquo;s cost optimization strategy focuses on maximizing AWS Free Tier usage and managed services. The figure of 138.06 USD/month is a realistic cost for long-term operation (after 12 months) of a complete, scalable, and highly secure e-commerce system.\nCosts for Elastic Beanstalk are broken down into the resources it manages: Amazon EC2 (1 instance t3a.small) costing 19.15 USD/month and Elastic Load Balancing (1 Application Load Balancer) costing 18.69 USD/month. Amazon VPC service costs 46.02 USD/month, this is the cost for 1 NAT Gateway, a mandatory component for Elastic Beanstalk\u0026rsquo;s Private Subnet security design. Regarding database and cache, Amazon RDS for SQL Server (Express Edition version on db.t3.micro) costs 25.86 USD/month, and Amazon ElastiCache (cache.t4g.micro) is 17.52 USD/month. Security and CI/CD services like WAF (7.20 USD/month), CloudFront (2.43 USD/month), Route 53 (0.90 USD/month), S3 (0.17 USD/month), and CodeBuild (0.12 USD/month) make up the remaining costs.\nThe most important cost optimization strategy is leveraging AWS Free Tier in the first 12 months. Although the total estimate is 138.06 USD/month, many core services herein (including EC2 t3a.small, RDS db.t3.micro, ElastiCache t4g.micro, S3, and CloudFront) are within the Free tier free for 12 months. Therefore, the actual operating cost in the first year will be significantly lower, mainly consisting of costs for NAT Gateway (46.02 USD) and WAF (7.20 USD).\nRegarding ROI calculation, and initial investment is nearly zero (as infrastructure costs are covered within Free Tier and development costs are the group\u0026rsquo;s effort during the internship). Profit is almost immediate, as the solution directly solves revenue loss problems (from manual inventory management) and missed market opportunities (due to offline-only sales) stated in the Problem Statement (Part 1). Therefore, ROI (return on investment) is very high.\n7. Risk Assessment Some potential risks lie in three areas: Technical, Business, and Operational. Thus a mitigation plan and contingency plan have been prepared for high-impact risks.\nTechnical: System Overload (Performance Bottleneck):\nImpact: High | Probability: Medium This is the risk of the system being slow or crashing when there is high traffic volume. The group\u0026rsquo;s mitigation strategy is to use ElastiCache to offload queries for RDS, configure Auto Scaling (in Elastic Beanstalk) with reasonable triggers (e.g., CPU \u0026gt; 70%), and use CloudFront to cache static files. The contingency plan is to use CloudWatch Alarms for immediate alerts, and if RDS overloads, the group will perform vertical scaling of the RDS instance immediately. Business: Low User Adoption:\nImpact: High | Probability: Medium This is the risk that mini-market owners find the solution too complex and do not use it. To mitigate this risk, the group will stick to a simple frontend design (Bootstrap), gather shop owner feedback early from Phase 2, and provide instruction manuals. The contingency plan is if adoption is low after deployment, the group will perform an additional Sprint (Phase 5) to prioritize adjusting features based on gathered feedback. Operational: Data Loss / Breach:\nImpact: Critical | Probability: Low The group\u0026rsquo;s mitigation strategy is to configure RDS for automatic daily backups, place RDS and Beanstalk in Private Subnet, use WAF to block attacks, and manage connection strings via Beanstalk environment variables. The contingency plan is if data is lost, the group will perform Point-in-Time Recovery (PITR) immediately from RDS backup. 8. Expected Outcomes The goal of this solution is to directly address the problems stated in the Problem Statement section. Regarding business metrics, the group expects to reduce inventory management errors by 90% (compared to Excel/notebooks), reduce checkout time at the counter by 50%, and achieve at least 20% new revenue from online channels in the first 6 months. Regarding technical metrics, the goal is to maintain 99.9% uptime, ensure average page load time under 2 seconds (thanks to CloudFront and ElastiCache), and stable deployment frequency via CI/CD pipeline.\nBenefits are expected to increase over time. In the short-term (0-6 months), mini-market owners will see immediately improved user experience and significantly improved operations (automated inventory management). In the medium-term (6-18 months), value comes from market expansion (reaching online customers) and starting to collect valuable business data. Long-term value and strategic capabilities gained are the ability to make data-driven decisions (e.g., knowing which products sell well) and easy system scalability (adding more new stores) thanks to Solution Architecture on Elastic Beanstalk and RDS.\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.6-cicd/5.6.2-codepipeline/","title":"Set up CodePipeline","tags":[],"description":"","content":" Access CodePipeline \u0026gt; Create pipeline\nCategory: Select Build custom pipeline\nSettings: Select New service role\nSource Stage: Select GitHub (via GitHub App) \u0026gt; Connect to GitHub \u0026gt; Select Repo and branch that you deploy to cloud\nBuild Stage: Select AWS CodeBuild \u0026gt; Select project MiniMarket-Build just created\nTest Stage: Click Skip test stage\nDeploy Stage:\nProvider: AWS Elastic Beanstalk Application name: MiniMarket-App Environment name: Select running environment Click Create pipeline\nIf Deploy step has Permission error, go to IAM Role of CodePipeline and grant permission AdministratorAccess-AWSElasticBeanstalk\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.7-security/5.7.2-waf/","title":"Set up Firewall (WAF)","tags":[],"description":"","content":" Access WAF \u0026amp; Shield \u0026gt; Protection packs (webs ACLs) \u0026gt; Create protection pack (web ACL) App category: E-commerce \u0026amp; transaction platforms App focus: Both API and web Add resources \u0026gt; Add CloudFront or Amplify resources \u0026gt; Select CloudFront distribution created in previous section Choose initial protections \u0026gt; Build your own pack from all of the protections AWS WAF offers \u0026gt; AWS-managed rule group:\nAdd Core rule set (Block bot, bad IP) Add SQL database (Block SQL Injection) Testing: Access URL: https://[domain]/?id=1 OR 1=1. If receiving error 403 Forbidden, WAF is active.\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Windows workloads on AWS. AWS Directory Services. Building a High Availability system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Windows workloads on AWS 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Managed Microsoft AD 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ 4 -Highly Available architectures 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ 5 - Multi-AZ \u0026amp; Fault tolerance 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ 6 - Case Study Review 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Deployed directory services. Designed a highly available system. "},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Design full AWS architecture for mini-market 11/17/2025 11/17/2025 3 - Deploy backend/API onto EC2 or ECS 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 - Set up database (RDS or DynamoDB) + API connectivity 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ 5 - Integrate S3 + CloudFront + IAM + CloudWatch 11/20/2025 11/20/2025 https://cloudjourney.awsstudygroup.com/ 6 - System testing + debugging + writing technical documentation 11/21/2025 11/21/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Completed full AWS architecture design for the mini-market project.\nSuccessfully deployed backend/API to cloud infrastructure.\nImplemented database layer and established secure connectivity.\nIntegrated S3 + CloudFront for static assets and performance.\nAdded CloudWatch logs and metrics for monitoring.\nFinal system tested, debugged, and fully operational.\nTechnical documentation prepared for final submission\n"},{"uri":"https://lvckio.github.io/Ckio1164/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.5-app/5.5.3-app-config/","title":"Configure Environment Variables","tags":[],"description":"","content":"To enable the application to connect to Database and Redis, we do not hardcode in the code but use Environment Variables\nGo to Beanstalk Environment \u0026gt; Configuration \u0026gt; Updates, monitoring, and logging \u0026gt; Edit\nScroll down to Environment properties section\nAdd the following variables:\nName: ConnectionStrings__DefaultConnection\nValue: Server=sql-shop-db\u0026hellip;.rds.amazonaws.com;Database=MiniMarketDB;User Id=admin;Password=PASSWORD YOU SET;TrustServerCertificate=True; Name: ConnectionStrings__RedisConnection\nValue: webapp.redis.cache\u0026hellip;:6379 Name: VnPay__IPNUrl\nValue: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayIPN Name: VnPay__ReturnUrl\nValue: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayReturn Click Apply. Server will restart to apply new configuration "},{"uri":"https://lvckio.github.io/Ckio1164/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"“AWS Security Governance \u0026amp; Automation” Report Event Purpose Introduce the vision of Cloud Club and the importance of community. Share methods for centralized Identity Management and account security. Guide on Multi-Layer Security Visibility strategy. Automate security incident response process (Automated Alerting) with EventBridge. Highlights Identity \u0026amp; Access Management (IAM \u0026amp; Governance) Single Sign-On (SSO): \u0026ldquo;One login, multiple systems\u0026rdquo; mechanism. Instead of creating many scattered IAM Users, SSO allows centralized identity management, helping users log in once to access multiple AWS accounts/applications. Service Control Policies (SCPs): A type of Organizational Policy used to set up \u0026ldquo;guardrails\u0026rdquo;. SCPs limit maximum permissions for member accounts in AWS Organization. Credentials Spectrum: Long-term: IAM User Access Keys (do not expire) → High risk, need to limit usage. Short-term: IAM Roles, STS tokens (expire after 15 minutes - 36 hours) → Higher security, is best practice. MFA (Multi-Factor Authentication): Mandatory security layer for every account. Multi-Layer Security Visibility IAM Access Analyzer: Tool helping detect resources (S3, KMS, IAM Roles\u0026hellip;) being publicly shared or shared with untrusted accounts. Event Classification (Logging): Management Events: Who did what to resources? (Ex: Create EC2, Delete S3 Bucket). Data Events: Who accessed the data? (Ex: GetObject S3, Invoke Lambda). Network Activity Events: Monitor VPC network traffic. Automation \u0026amp; Alerting Amazon EventBridge: Real-time Event processing center. Supports Cross-account Event: Receive events from child accounts to the central security account. Automated Alerting: Automatically send alerts when abnormal behavior is detected. Detection as Code: Convert threat detection logic into code (Infrastructure as Code). Use CloudTrail Lake queries to query history. Version control for security rules. Network Security Common Attack Vectors: Common network attack vectors (DDoS, SQL Injection, Man-in-the-middle\u0026hellip;). AWS Layered Security: Defense in Depth strategy - protecting from the outer layer (Edge), network layer (VPC), to application and data layers. What I Learned Modern Security Mindset Identity is the new perimeter: In Cloud environment, identity management (IAM/SSO) is more important than traditional firewalls. Zero Trust: Trust no one, always authenticate (MFA) and grant least privilege. Governance Strategy Shift from Long-term to Short-term: Understand clearly the risks of long-term Access Keys and the importance of shifting to Temporary Credentials. Governance at Scale: Use SCPs to manage hundreds of AWS accounts consistently instead of manual configuration for each. Automation Techniques Event-Driven Security: Instead of manual log review (passive), use EventBridge to react immediately (active) when security incidents occur. Application to Work Review IAM: Check and disable old IAM Access Keys, enforce MFA for the whole team. Deploy Access Analyzer: Activate immediately to scan if any S3 bucket is mistakenly public. Set up alerts: Create simple EventBridge rules to send notifications to Slack/Email when someone logs in with Root account or changes Security Group. Learn about CloudTrail: Configure CloudTrail to record both Management and Data events for critical resources. Event Experience The 3rd event brought a very deep perspective on Security \u0026amp; Governance aspects - an area often overlooked in development but vital for businesses.\nRisk Awareness The presentation on Credentials Spectrum startled me into realizing how dangerous the habit of using IAM Users with Long-term keys is. Shifting to Short-term credentials is mandatory. The Power of Automation I was very impressed with the concept of \u0026ldquo;Detection as Code\u0026rdquo;. Managing security rules like source code helps operations become transparent and easier to control. The EventBridge demo showed excellent real-time response capabilities, minimizing the time attackers can cause harm in the system. Multi-layered Defense Mindset Understood better about AWS Layered Security, security is not just a door but multiple barrier layers coordinating from Network to Identity.\nThis event changed my mindset from \u0026ldquo;Make it run\u0026rdquo; to \u0026ldquo;Make it safe and compliant\u0026rdquo;. Security is not a barrier, but a foundation for sustainable development.\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.4-data/5.4.3-elasticache/","title":"Initialize ElastiCache Redis","tags":[],"description":"","content":" Access ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group Name: redis-private-group Subnets: Select 2 Private Subnets Go to Redis OSS caches \u0026gt; Create cache At Cluster settings screen: Engine: Select Redis OSS Deployment option: Select Node-based cluster Creation method: Select Cluster cache (Configure and create a new cluster) Cluster mode: Select Disabled (Simple mode, 1 Shard) At Location screen:\nLocation: AWS Cloud Multi-AZ: Uncheck (Enable) Note: Disable this feature to save costs for Lab environment Auto-failover: Uncheck (Enable) At Cache settings screen:\nEngine version: Leave default (Ex: 7.1) Port: 6379 Node type: Select t3 family \u0026gt; Select cache.t3.micro Number of replicas: Enter 0 (We only need 1 primary node, no replica node needed) At Connectivity screen: Network type: IPv4 Subnet groups: Select Choose existing subnet group \u0026gt; Select redis-private-group just created At Advanced settings screen (Important): Encryption at rest: Enable (Default) Encryption in transit: Uncheck (Disable) Reason: Disabling encryption in transit simplifies connection from .NET code in internal VPC environment without configuring complex SSL certificates Selected security groups: Select Manage \u0026gt; select sg-redis-cache (Uncheck default) Scroll to the bottom and click Create 3. Get connection information Initialization process will take about 5-10 minutes\nWhen status changes to Available (Green) Click on Cluster name (webapp or name you set) At Overview tab, find Primary endpoint Copy this connection string (Ex: webapp.xxxx.cache.amazonaws.com) This Endpoint will be used to configure ConnectionStrings__RedisConnection environment variable for Elastic Beanstalk in the following steps.\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.3-network/","title":"Network Infrastructure Setup","tags":[],"description":"","content":"Overview In this section, we will build the network foundation for the MiniMarket application. A secure network architecture is a prerequisite to protect the application and data\nWe will design a VPC consisting of:\nPublic Subnet: Dedicated to components communicating directly with the Internet (Load Balancer, NAT Gateway). Private Subnet Dedicated to components requiring security (App Server, Database, Redis) Additionally, we will configure NAT Gateway to allow servers inside the Private Subnet to download updates and Docker Images from the Internet without exposing IP addresses externally\nContent Create VPC \u0026amp; Subnet Configure Internet \u0026amp; NAT Gateway Configure Route Table "},{"uri":"https://lvckio.github.io/Ckio1164/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.4-data/","title":"Data Layer Deployment","tags":[],"description":"","content":"Overview Data is the most important asset of every system. Therefore we will set up the data layer (Data Layer) for MiniMarket with criteria: Maximum Security and High Performance\nWe will deploy two core services:\nAmazon RDS (Relational Database Service): Use SQL Server to store business data (Products, Orders, Users). Database will be placed in Private Subnet to prevent direct access from the Internet Amazon ElastiCache (Redis): Use Redis as cache memory (In-memory Cache) to store Login Sessions and offload queries for the main Database Content Set up Security Groups for DB \u0026amp; Cache Initialize Amazon RDS (SQL Server) Initialize Amazon ElastiCache (Redis) "},{"uri":"https://lvckio.github.io/Ckio1164/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During the internship, I had the opportunity to participate in four AWS-related technology events.\nEach event provided valuable knowledge, hands-on insights, and memorable experiences that helped me strengthen my understanding of Cloud, DevOps, AI/ML, and Security on AWS.\nBelow is a summary of the events I attended:\nEvent 1 Event Name: “GenAI \u0026amp; Bedrock AI Services Workshop”\nTime: 19:30 – October 16, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML/GenAI on AWS\nTime: 8:30 – November 15, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DevOps on AWS\nTime: 8:30 – November 17, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Well-Architected Security Pillar\nTime: 8:30 – November 29, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.5-app/","title":"Application Deployment","tags":[],"description":"","content":"Overview After having network and data infrastructure, the next step is to bring .NET Core application source code to Cloud. Instead of managing each EC2 virtual server manually, we will use the Platform-as-a-Service (PaaS) platform which is AWS Elastic Beanstalk\nGoals of this module:\nContainerization: Package MiniMarket application into Docker Container to ensure uniform running environment (Dev = Prod) Deployment: Deploy Container to Elastic Beanstalk. The system will automatically provision EC2, configure Load Balancer and Auto Scaling Group Connectivity: Configure so application connects securely to RDS and Redis via Environment Variables Content Package application with Docker Initialize Elastic Beanstalk Environment Configure Database \u0026amp; Redis connection "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying Cloud-Native MiniMarket Application on AWS Overview This workshop provides a comprehensive guide on re-platforming the MiniMarket e-commerce application (developed on the .NET Core platform) from a local environment to the AWS cloud infrastructure using a Cloud Native architecture.\nWe will not merely rent a virtual server (EC2) to run code. Instead, we will build a distributed system that is highly scalable, secure, and automated based on managed services.\nWe will establish a Multi-tier architecture consisting of core components:\nCompute: Use AWS Elastic Beanstalk combined with Docker to simplify application deployment and management, supporting automatic Auto Scaling based on traffic. Data \u0026amp; Caching: Migrate from local SQL Server to Amazon RDS (placed in Private Subnet) to ensure data security. Simultaneously, deploy Amazon ElastiCache (Redis) to manage User Sessions, ensuring high performance for the application. Networking \u0026amp; Security: Use VPC along with Public/Private Subnet and NAT Gateway for secure outbound connection, and protect the application against attacks using AWS WAF combined with CloudFront. DevOps: Build a CI/CD process using AWS CodePipeline and CodeBuild, allowing automation of the process from committing code to GitHub until the application runs in the Production environment Content Workshop Overview Prerequisites Network Infrastructure Setup (VPC, NAT, Security Groups) Data Layer Deployment (RDS \u0026amp; Redis) Application Deployment with Elastic Beanstalk \u0026amp; Docker Automation with CI/CD Pipeline Optimization and Security(S3, CloudFront, WAF) Monitoring (CloudWatch) Resource Cleanup "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.6-cicd/","title":"CI/CD Automation","tags":[],"description":"","content":"Overview In the Cloud Native environment, manual deployment (Manual Deployment) is risky and time-consuming. This module will guide you to build a fully automated CI/CD (Continuous Integration / Continuous Deployment) process\nThe process operates as follows:\nSource: Developer pushes code (Push) to GitHub Build: AWS CodePipeline detects changes and activates AWS CodeBuild. CodeBuild will package the Docker Image and push to the Amazon ECR repository Deploy: Pipeline automatically commands Elastic Beanstalk to update the latest version from ECR without service interruption Content Create Build Project with AWS CodeBuild Set up AWS CodePipeline "},{"uri":"https://lvckio.github.io/Ckio1164/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://lvckio.github.io/Ckio1164/7-feedback/","title":"Feedback &amp; Suggestions","tags":[],"description":"","content":" ⚠️ Note: The content below is for reference only. Please revise it to match your personal experience.\nHere, I would like to share my personal reflections and feedback about the First Cloud Journey program to help the FCJ team further improve future batches.\nOverall Evaluation 1. Working Environment The working environment is friendly, supportive, and very comfortable. FCJ members are always willing to help whenever I face difficulties. The workspace is clean and organized, which helps me stay focused. I believe adding more team-bonding activities would enhance the connection between interns.\n2. Support from Mentor / Team Admin My mentor provides clear explanations, detailed guidance, and always encourages me to think independently before asking questions. The admin team is efficient in handling documents and ensures I receive all necessary information promptly.\n3. Relevance of Work to Academic Major The assigned tasks closely relate to what I studied at university while expanding my knowledge in practical areas such as AWS Cloud, DevOps, and system architecture. This combination helps reinforce both theory and hands-on skills.\n4. Learning \u0026amp; Skill Development Opportunities Throughout the program, I improved skills in teamwork, technical analysis, communication, and project management. The workshops and internal training sessions also provided valuable insights into real-world cloud practices.\n5. Company Culture \u0026amp; Team Spirit The company culture is positive and respectful. Everyone supports one another, especially during high workloads. Despite being an intern, I always felt like a valued member of the team.\nAdditional Questions • What was most satisfying during the internship? Having the opportunity to work on real cloud-based projects and receiving continuous guidance from my mentor.\n• What should the company improve for future interns? More structured weekly check-ins and additional social activities for interns.\n• Would I recommend this internship to others? Why? Yes. FCJ provides great mentorship, real project experience, and a supportive environment for learning cloud technologies.\nSuggestions \u0026amp; Expectations Organize more technical workshops or hands-on lab sessions. Create cross-team mini-projects to help interns improve collaboration. I hope to continue participating in FCJ programs in the future if possible. Overall, this internship gave me valuable experience, strengthened my technical foundation, and helped shape my professional mindset.\n"},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.7-security/","title":"Optimization &amp; Security","tags":[],"description":"","content":"Resource Cleanup Overview A Production system needs not only to \u0026ldquo;run\u0026rdquo; but also to \u0026ldquo;run fast\u0026rdquo; and be \u0026ldquo;secure\u0026rdquo;. In this part, we will refine the MiniMarket architecture\nImplementation items:\nOffloading Static Assets: Transfer all product images from Web Server to Amazon S3 and distribute via Amazon CloudFront (CDN) to accelerate global page load speed and offload the server Security Hardening: Deploy AWS WAF (Web Application Firewall) in front of CloudFront to protect the application from common attacks such as SQL Injection and XSS Content Configure S3 and CloudFront Set up AWS WAF "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.8-monitoring/","title":"Monitoring &amp; Operations","tags":[],"description":"","content":"Overview A Production system cannot be considered complete without Monitoring and Alerting capabilities. You cannot sit watching the screen 24/7 to check if the Server is still alive\nIn this module, we will set up monitoring and alerting for MiniMarket using AWS operations management services:\nAmazon CloudWatch: Collect metrics (Metrics) from EC2, RDS, ELB Amazon SNS (Simple Notification Service): Notification service. We will use it to send Emails to administrators when the system encounters issues We will set up a CloudWatch Alarm to monitor Web Server CPU. If CPU exceeds 70% (sign of overload or attack), the system will automatically trigger SNS to send emergency alert Emails\nContent Set up CloudWatch Alarms \u0026amp; SNS "},{"uri":"https://lvckio.github.io/Ckio1164/5-workshop/5.9-cleanup/","title":"Resource Cleanup","tags":[],"description":"","content":"Overview Congratulations on successfully deploying MiniMarket on AWS!\nHowever, our work does not end there. The final step and also the most important step to protect your \u0026ldquo;wallet\u0026rdquo; is Resource Cleanup\nThe services we deployed such as NAT Gateway, Elastic Load Balancer, RDS, ElastiCache are all billed by the hour, whether you use them or not. If you forget to delete, the month-end bill can be very high\nWe will go through the system Decommissioning process in the correct order to ensure no resources are left behind causing hidden costs\nContent Safe resource deletion process "},{"uri":"https://lvckio.github.io/Ckio1164/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://lvckio.github.io/Ckio1164/tags/","title":"Tags","tags":[],"description":"","content":""}]